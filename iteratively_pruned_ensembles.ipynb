{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code base used for performing iteratively pruned ensembles from the models fine-tuned from modlaity-specific knowledge transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from keras.layers import Add, Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Conv2D, Concatenate\n",
    "from keras.layers import SeparableConv2D\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import statistics \n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "import numpy as np\n",
    "from keras.backend import tensorflow_backend\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import ZeroPadding2D, GlobalAveragePooling2D\n",
    "import time\n",
    "from scipy import interp\n",
    "import cv2\n",
    "import imutils\n",
    "import pickle\n",
    "import struct\n",
    "import shutil\n",
    "import numpy as np\n",
    "import zlib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import classification_report,confusion_matrix, roc_curve, auc, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "from sklearn.utils import class_weight\n",
    "from keras.regularizers import l2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "import random\n",
    "from shutil import copyfile\n",
    "from tensorflow.python.framework import ops\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras import applications\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.losses import *\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np \n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from PIL import Image\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras import backend as keras\n",
    "try:\n",
    "    from itertools import izip as zip\n",
    "except ImportError: # will be 3.x series\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert dicom to png\n",
    "\n",
    "def dcm2png(input_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Becareful all output images are gray image with 8 bit\n",
    "    :param input_dir: dcm file directory\n",
    "    :param output_dir: save directory\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(input_dir):\n",
    "        raise ValueError(\"Input dir is not found!\")\n",
    "\n",
    "    if not os.path.isdir(output_dir):\n",
    "        raise ValueError(\"Out dir is not found!\")\n",
    "\n",
    "    img_list = [f for f in os.listdir(input_dir)\n",
    "                if f.split('.')[-1] == 'dcm' or f.split('.')[-1] == 'jpeg'] \n",
    "    for n, f in enumerate(img_list):\n",
    "        \n",
    "        if f.split(\".\")[-1] == \"dcm\":\n",
    "            dcm_file = input_dir + f\n",
    "            ds = pydicom.dcmread(dcm_file)\n",
    "            pixel_array_numpy = ds.pixel_array\n",
    "            pixel_array_numpy = cv2.normalize(pixel_array_numpy,\n",
    "                                              None,\n",
    "                                              alpha=0,\n",
    "                                              beta=255,\n",
    "                                              norm_type=cv2.NORM_MINMAX,\n",
    "                                              dtype=cv2.CV_8UC1)\n",
    "            pixel_array_numpy = cv2.resize(pixel_array_numpy,\n",
    "                                           (224,224))\n",
    "            img_file = output_dir + f.replace('.dcm', '.png')\n",
    "            cv2.imwrite(img_file, pixel_array_numpy)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            if f.split(\".\")[-1] == \"jpeg\":\n",
    "                image_file = input_dir + f\n",
    "                pixel_array_numpy = cv2.imread(image_file)\n",
    "                pixel_array_numpy = cv2.cvtColor(pixel_array_numpy, cv2.COLOR_BGR2GRAY)\n",
    "                pixel_array_numpy = cv2.resize(pixel_array_numpy,\n",
    "                                               (224,224))\n",
    "                image_file = output_dir + f.replace('.jpeg', '.png')\n",
    "                cv2.imwrite(image_file, pixel_array_numpy)\n",
    "                \n",
    "                if n % 50 == 0:\n",
    "                    print('{} image processed'.format(n))\n",
    "\n",
    "#usage\n",
    "#dcm2png(\"cxr_pneumonia_GT/test/PNEUMONIA/\", \"ped_pneumonia_256/test/PNEUMONIA/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin U-Net based semantic segmentation to genrate lung masks for the input CXRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define different loss functions\n",
    "\n",
    "def jaccard_distance(y_true, y_pred, smooth=100):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (\n",
    "                K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "#another implementation of dice coefficient from https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return dice_loss(y_true, y_pred) + binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "def bce_jac_loss(y_true, y_pred):\n",
    "    return jaccard_distance(y_true, y_pred, smooth=100) + binary_crossentropy(y_true, y_pred)\n",
    "   \n",
    "def iou(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "\n",
    "    \n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "def jacard_coef_loss(y_true, y_pred):\n",
    "    return -jacard_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def threshold_binarize(x, threshold=0.5):\n",
    "    ge = tf.greater_equal(x, tf.constant(threshold))\n",
    "    y = tf.where(ge, x=tf.ones_like(x), y=tf.zeros_like(x))\n",
    "    return y\n",
    "\n",
    "\n",
    "def iou_thresholded(y_true, y_pred, threshold=0.5, smooth=1.):\n",
    "    y_pred = threshold_binarize(y_pred, threshold)\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the UNET model architecture with dropout of 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilatedunet(pretrained_weights = None,input_size = (256,256,1)): #generate 256x256 lung masks\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.2)(conv4)  #empirically determine the best value\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.2)(conv5) \n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', \n",
    "                 dilation_rate=2, kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = Add()([drop4,up6])\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', \n",
    "                 dilation_rate=2, kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = Add()([conv3,up7])\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', \n",
    "                 dilation_rate=2, kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = Add()([conv2,up8])\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', \n",
    "                 dilation_rate=2, kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = Add()([conv1,up9])\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr=1e-4), loss=[bce_dice_loss], \n",
    "                  metrics=[iou, iou_thresholded,'accuracy']) #choose the best loss function\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the data format and the functions to train and test with the data using image generators. Make sure to use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same. If you want to visualize the results of generator, set save_to_dir = \"your path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a color dictionary\n",
    "A = [128,128,128]\n",
    "B = [128,0,0]\n",
    "C = [192,192,128]\n",
    "D = [128,64,128]\n",
    "E = [60,40,222]\n",
    "F = [128,128,0]\n",
    "G = [192,128,128]\n",
    "H = [64,64,128]\n",
    "I = [64,0,128]\n",
    "J = [64,64,0]\n",
    "K = [0,128,192]\n",
    "Unlabelled = [0,0,0]\n",
    "\n",
    "COLOR_DICT = np.array([A, B, C, D, E, F, G, H, I, J, K, Unlabelled])\n",
    "\n",
    "def adjustData(img,mask,flag_multi_class,num_class):\n",
    "    if(flag_multi_class):\n",
    "        img = img / 255\n",
    "        mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n",
    "        new_mask = np.zeros(mask.shape + (num_class,))\n",
    "        for i in range(num_class):\n",
    "            new_mask[mask == i,i] = 1\n",
    "        new_mask = np.reshape(new_mask,(new_mask.shape[0],\n",
    "                                        new_mask.shape[1]*new_mask.shape[2],\n",
    "                                        new_mask.shape[3])) if flag_multi_class else np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n",
    "        mask = new_mask\n",
    "    elif(np.max(img) > 1):\n",
    "        img = img / 255\n",
    "        mask = mask /255\n",
    "        mask[mask > 0.5] = 1\n",
    "        mask[mask <= 0.5] = 0\n",
    "    return (img,mask)\n",
    "\n",
    "\n",
    "def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"grayscale\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (256,256),seed = 1): \n",
    "\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    for (img,mask) in train_generator:\n",
    "        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
    "        yield (img,mask)\n",
    "        \n",
    "def valGenerator(batch_size,val_path,image_folder,mask_folder,aug_dict,image_color_mode = \"grayscale\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (256,256),seed = 1): \n",
    "\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        val_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        val_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    val_generator = zip(image_generator, mask_generator)\n",
    "    for (img,mask) in val_generator:\n",
    "        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
    "        yield (img,mask)\n",
    "\n",
    "def testGenerator(test_path,target_size = (256,256),flag_multi_class = False,as_gray = True): \n",
    "    for filename in os.listdir(test_path):\n",
    "        img = io.imread(os.path.join(test_path,filename),as_gray = as_gray) \n",
    "        img = img / 255.\n",
    "        img = trans.resize(img,target_size)\n",
    "        img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
    "        img = np.reshape(img,(1,)+img.shape)\n",
    "        yield img\n",
    "\n",
    "\n",
    "def labelVisualize(num_class,color_dict,img):\n",
    "    img = img[:,:,0] if len(img.shape) == 3 else img\n",
    "    img_out = np.zeros(img.shape + (3,))\n",
    "    for i in range(num_class):\n",
    "        img_out[img == i,:] = color_dict[i]\n",
    "    return img_out / 255\n",
    "\n",
    "\n",
    "def saveResult(save_path,npyfile,test_path, flag_multi_class = False,num_class = 2):\n",
    "    file_names = os.listdir(test_path)\n",
    "    for i,item in enumerate(npyfile):\n",
    "        img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
    "        io.imsave(os.path.join(save_path,file_names[i]),img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the dilated UNET model\n",
    "\n",
    "data_gen_args = dict(rotation_range=10.,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=5,\n",
    "                    zoom_range=0.3,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest') \n",
    "myGene = trainGenerator(2,'C:/Users/data/train',\n",
    "                        'image','label',data_gen_args,save_to_dir = None) #batch size of 2 used here\n",
    "\n",
    "\n",
    "model = dropoutunet()\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='loss', patience=15, verbose=1, min_delta=1e-4,\n",
    "                           mode='min'),\n",
    "             ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=1,\n",
    "                               epsilon=1e-4, mode='min'),\n",
    "             ModelCheckpoint(monitor='loss', \n",
    "                             filepath='C:/Users/trained_model/dilatedunet.hdf5', \n",
    "                             save_best_only=True,\n",
    "                             mode='min', verbose = 1)]\n",
    "model.fit_generator(generator=myGene,steps_per_epoch=217, epochs=200, callbacks=callbacks,\n",
    "                    verbose=1) #steps_per_epoch=training samples/batchsize + 1 if not absolutely divisible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"C:/Users/data/test\" \n",
    "save_path = \"C:/Users/data/membrane/result\" \n",
    "\n",
    "data_gen_args = dict(rotation_range=10.,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=5,\n",
    "                    zoom_range=0.3,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest') \n",
    "\n",
    "testGene = testGenerator(test_path)\n",
    "model = dropoutunet()\n",
    "model.load_weights(\"C:/Users/trained_model/dropoutunet.hdf5\")\n",
    "results = model.predict_generator(testGene,135,verbose=1, workers=1, use_multiprocessing=False) \n",
    "#steps per epoch is the no. of samples in test image.\n",
    "saveResult(save_path, results, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postprocessing with the mask and image: This script helps to postprocess the images with the mask generated through the UNET and relax the boundaries by 5% on top, bottom, left, and right, and store the bounding box cordinates to a csv file. The cropped bounding box images are stored to a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom function to generate bounding boxes\n",
    "def generate_bounding_box(image_dir: str, #containing images\n",
    "                          mask_dir: str, #containing masks, images have same name as original images\n",
    "                          dest_csv: str, #CSV file to write the bounding box coordinates\n",
    "                          crop_save_dir: str): #save the cropped bounding box images\n",
    "    \"\"\"\n",
    "    the orginal images are resized to 256x256\n",
    "    the output crops are resized to 256x256\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(mask_dir):\n",
    "        raise ValueError(\"mask_dir not existed\")\n",
    "\n",
    "    case_list = [f for f in os.listdir(mask_dir) if f.split(\".\")[-1] == 'png'] #all mask images are png files\n",
    "\n",
    "    with open(dest_csv, 'w', newline='') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "\n",
    "        for j, case_name in enumerate(case_list):\n",
    "            mask = cv2.imread(mask_dir + case_name)\n",
    "            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.imread(image_dir + case_name, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (256,256)) #original images are resized to 256x256\n",
    "            if mask is None or image is None:\n",
    "                raise ValueError(\"The image can not be read: \" + case_name)\n",
    "\n",
    "            reduce_col = np.sum(mask, axis=1)\n",
    "            reduce_row = np.sum(mask, axis=0)\n",
    "            # many 0s add up to none zero, we need to threshold it\n",
    "            reduce_col = (reduce_col >= 255)*reduce_col\n",
    "            reduce_row = (reduce_row >= 255)*reduce_row\n",
    "            first_none_zero = None\n",
    "            last_none_zero = None\n",
    "\n",
    "            last = 0\n",
    "            for i in range(reduce_col.shape[0]):\n",
    "                current = reduce_col[i]\n",
    "                if last == 0 and current != 0 and first_none_zero is None:\n",
    "                    first_none_zero = i\n",
    "\n",
    "                if current != 0:\n",
    "                    last_none_zero = i\n",
    "\n",
    "                last = reduce_col[i]\n",
    "\n",
    "            up = first_none_zero\n",
    "            down = last_none_zero\n",
    "\n",
    "            first_none_zero = None\n",
    "            last_none_zero = None\n",
    "            last = 0\n",
    "            for i in range(reduce_row.shape[0]):\n",
    "                current = reduce_row[i]\n",
    "                if last == 0 and current != 0 and first_none_zero is None:\n",
    "                    first_none_zero = i\n",
    "\n",
    "                if current != 0:\n",
    "                    last_none_zero = i\n",
    "\n",
    "                last = reduce_row[i]\n",
    "\n",
    "            left = first_none_zero\n",
    "            right = last_none_zero\n",
    "\n",
    "            if up is None or down is None or left is None or right is None:\n",
    "                raise ValueError(\"The border is not found: \" + case_name)\n",
    "            \n",
    "            # new coordinates for image which is 1 times of mask, mask images are 256x256 \n",
    "            #so need to multiply 1 times to get 256x256, and relaxing the borders by 5% on all directions\n",
    "            up_down_loose = int(1 * (down - up + 1) * 0.05)\n",
    "            image_up = 1 * up - up_down_loose\n",
    "            if image_up < 0:\n",
    "                image_up = 0\n",
    "            image_down = 1*(down+1)+up_down_loose\n",
    "            if image_down > image.shape[0] + 1:\n",
    "                image_down = image.shape[0]\n",
    "\n",
    "            left_right_loose = int(1 * (right - left) * 0.05)\n",
    "            image_left = 1 * left - left_right_loose\n",
    "            if image_left < 0:\n",
    "                image_left = 0\n",
    "            image_right = 1*(right + 1)+left_right_loose\n",
    "            if image_right > image.shape[1] + 1:\n",
    "                image_right = image.shape[1]\n",
    "\n",
    "            crop = image[image_up: image_down, image_left: image_right]\n",
    "            crop = cv2.resize(crop,(256,256)) #the cropped image is resized to 256x256\n",
    "\n",
    "            cv2.imwrite(crop_save_dir + case_name, crop) # cropped images saved to crop directory\n",
    "\n",
    "            # write new csv\n",
    "            crop_width = image_right - image_left + 1\n",
    "            crop_height = image_down - image_up + 1\n",
    "\n",
    "            csv_writer.writerow([case_name,\n",
    "                                 image_left,\n",
    "                                 image_up,\n",
    "                                 crop_width,\n",
    "                                 crop_height]) #writes xmin, ymin, width, and height\n",
    "\n",
    "            if j % 50 == 0:\n",
    "                print(j, \" images are processed!\")\n",
    "\n",
    "#train-normal\n",
    "generate_bounding_box(\"C:/Users/data/test/\",\n",
    "                      \"C:/Users/result/mask_1/\",\n",
    "                      'C:/Users/result/bounding_box.csv',\n",
    "                      \"C:/Users/result/cropped1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the case if the CXR images have ground truth ROI annotations, we need to downscale and store these values after bounding box cropping. In this regard, we can use the following custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bounding_box(image_dir: str,\n",
    "                          mask_dir: str,\n",
    "                          orgin_csv: str,\n",
    "                          dest_csv: str,\n",
    "                          crop_save_dir: str,\n",
    "                          show_results: bool=False):\n",
    "    if not os.path.isdir(mask_dir):\n",
    "        raise ValueError(\"mask_dir not existed\")\n",
    "\n",
    "    csv_file_data = []\n",
    "    with open(orgin_csv, 'r') as csvFile:\n",
    "        reader = csv.reader(csvFile)\n",
    "        for row in reader:\n",
    "            csv_file_data.append(row)\n",
    "\n",
    "    with open(dest_csv, 'w', newline='') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "\n",
    "        for j, row in enumerate(csv_file_data[1:]):\n",
    "            case_name = row[0] + '.png' #all mask files are png file type\n",
    "            mask = cv2.imread(mask_dir + case_name)\n",
    "            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.imread(image_dir + case_name, cv2.COLOR_BGR2GRAY)\n",
    "            #original images are resized to 256x256, comment this if you want to keep the original image resolution\n",
    "            image = cv2.resize(image,(256,256)) \n",
    "            if mask is None or image is None:\n",
    "                raise ValueError(\"The image can not be read: \" + case_name)\n",
    "            \n",
    "            reduce_col = np.sum(mask, axis=1)\n",
    "            reduce_row = np.sum(mask, axis=0)\n",
    "            # many 0s add up to none zero, we need to threshold it\n",
    "            reduce_col = (reduce_col >= 255)*reduce_col\n",
    "            reduce_row = (reduce_row >= 255)*reduce_row\n",
    "            first_none_zero = None\n",
    "            last_none_zero = None\n",
    "\n",
    "            last = 0\n",
    "            for i in range(reduce_col.shape[0]):\n",
    "                current = reduce_col[i]\n",
    "                if last == 0 and current != 0 and first_none_zero is None:\n",
    "                    first_none_zero = i\n",
    "\n",
    "                if current != 0:\n",
    "                    last_none_zero = i\n",
    "\n",
    "                last = reduce_col[i]\n",
    "\n",
    "            up = first_none_zero\n",
    "            down = last_none_zero\n",
    "\n",
    "            first_none_zero = None\n",
    "            last_none_zero = None\n",
    "            last = 0\n",
    "            for i in range(reduce_row.shape[0]):\n",
    "                current = reduce_row[i]\n",
    "                if last == 0 and current != 0 and first_none_zero is None:\n",
    "                    first_none_zero = i\n",
    "\n",
    "                if current != 0:\n",
    "                    last_none_zero = i\n",
    "\n",
    "                last = reduce_row[i]\n",
    "\n",
    "            left = first_none_zero\n",
    "            right = last_none_zero\n",
    "            \n",
    "            if up is None or down is None or left is None or right is None:\n",
    "                raise ValueError(\"The border is not found: \" + case_name)\n",
    "            \n",
    "            # new coordinates for image which is 1 times of mask, mask images are 256x256, \n",
    "            #need to multiply 1 times to get 256x256, and relaxing the borders by 5% on all directions\n",
    "            \n",
    "            loose = int(1 * (down - up + 1) * 0.05) \n",
    "            # for example, if the original image resolution is \n",
    "            #1024x1024, new coordinates for image is 4 times of mask (256x256), \n",
    "            #need to multiply 4 times to 1024x1024, and relaxing the borders by 5% on all directions, \n",
    "            #i.e. int(4 * (down - up + 1) * 0.05)\n",
    "            image_up = 1 * up - loose #4\n",
    "            if image_up < 0:\n",
    "                image_up = 0\n",
    "            image_down = 1*(down+1)+loose #4\n",
    "            if image_down > image.shape[0] + 1:\n",
    "                image_down = image.shape[0]\n",
    "\n",
    "            loose2 = int(1 * (right - left + 1) * 0.05) #4\n",
    "            image_left = 1 * left - loose2 #4\n",
    "            if image_left < 0:\n",
    "                image_left = 0\n",
    "            image_right = 1*(right+1)+loose2 #4\n",
    "            if image_right > image.shape[1] + 1:\n",
    "                image_right = image.shape[1]\n",
    "\n",
    "            crop = image[image_up: image_down, image_left: image_right]\n",
    "            crop = cv2.resize(crop, (256,256)) #1024, 1024 before\n",
    "\n",
    "            # store images in normal or abnormal folder\n",
    "            if row[6] != 'Normal': #modify to your requirements\n",
    "                subfolder = 'abnormal/'\n",
    "            else:\n",
    "                subfolder = 'normal/'\n",
    "            cv2.imwrite(crop_save_dir + subfolder + case_name, crop)\n",
    "\n",
    "            # write new csv\n",
    "            crop_width = image_right - image_left + 1\n",
    "            crop_height = image_down - image_up + 1\n",
    "\n",
    "            if row[6] == \"Lung Opacity\":\n",
    "\n",
    "                y_scale_change = 256 / crop_height #1024 before\n",
    "                x_scale_change = 256 / crop_width # 1024 before, image size was 1024 by 1024\n",
    "\n",
    "                bbox_y = int(float(row[2])) \n",
    "                #new_y = int((bbox_y - image_up) * y_scale_change)\n",
    "                new_y = int((bbox_y/4 - image_up) * y_scale_change) #since resized to 256 from 1024, scale by 1/4\n",
    "\n",
    "                bbox_x = int(float(row[1]))\n",
    "                #new_x = int((bbox_x - image_left) * x_scale_change)\n",
    "                new_x = int((bbox_x/4 - image_left) * x_scale_change)  #since resized to 256 from 1024, scale by 1/4\n",
    "\n",
    "                bbox_width = int(float(row[3]))\n",
    "                bbox_height = int(float(row[4]))\n",
    "                #new_width = int(bbox_width * x_scale_change)\n",
    "                #new_height = int(bbox_height * y_scale_change)\n",
    "                new_width = int(bbox_width/4 * x_scale_change)  #since resized to 256 from 1024, scale by 1/4\n",
    "                new_height = int(bbox_height/4 * y_scale_change) #since resized to 256 from 1024, scale by 1/4\n",
    "\n",
    "                csv_writer.writerow([case_name,\n",
    "                                     image_left,\n",
    "                                     image_up,\n",
    "                                     crop_width,\n",
    "                                     crop_height,\n",
    "                                     new_x,\n",
    "                                     new_y,\n",
    "                                     new_width,\n",
    "                                     new_height,\n",
    "                                     row[6]])\n",
    "\n",
    "                else:\n",
    "                csv_writer.writerow([case_name,\n",
    "                                     image_left,\n",
    "                                     image_up,\n",
    "                                     crop_width,\n",
    "                                     crop_height,\n",
    "                                     \"\",\n",
    "                                     \"\",\n",
    "                                     \"\",\n",
    "                                     \"\",\n",
    "                                     row[6]])\n",
    "            \"\"\"\n",
    "            if show_results:\n",
    "                \n",
    "                # image distribution\n",
    "                plt.hist(mask.ravel(), 256, [0, 256])\n",
    "                mask = mask / 255\n",
    "                # Bounding box for image and mask\n",
    "                mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "                rect_mask = cv2.rectangle(mask, (left, up), (right, down), (0, 255, 0), 2)\n",
    "                rect_image = cv2.rectangle(image, (4 * left, 4 * up), (4 * right, 4 * down), (0, 255, 0), 2)\n",
    "                cv2.imshow(\"crop\", crop)\n",
    "                cv2.imshow(\"mask\", rect_mask)\n",
    "                cv2.imshow(\"image\", rect_image)\n",
    "            \"\"\"\n",
    "            if j % 100 == 0:\n",
    "                print(j, \" images are processed!\")\n",
    "\n",
    "\n",
    "generate_bounding_box(\"C:/Users/dcm_2_image/\",\n",
    "                      \"C:/Users/dcm_2_image_mask/\",\n",
    "                      'C:/Users/stage_2_train_labels.csv',\n",
    "                      'C:/Users/bounding_box_rsna_256.csv',\n",
    "                      \"C:/Users/rajaramans2/codes/crops/\",\n",
    "                      show_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed the orignal CXRs and cropped them to a size of the bounding box with 256x256x8 grayscale pixel resolution, we can begin training the models. Initially all models are trained and tested on the RSNA CXR collection to make them modality specific and help categorize the CXRs to normal and abnornmal categories.The top-3 performing modality-specific models are fine-tuned on a combination of pediatric CXR (normal, and bacterial pnuemonia) and COVID-19 CXR images (Twitter and Montreal collections) to categorize them in normal, bacterial pneiumonia, and COVID-19 viral pneunmonia categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% custom function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% path to input data\n",
    "train_data_dir = '...rsna_cxr/train'\n",
    "test_data_dir = '...rsna_cxr/test'\n",
    "img_width = 256\n",
    "img_height = 256\n",
    "channel = 3\n",
    "epochs = 32\n",
    "batch_size = 16 #vary this parameter depending on your GPU capacity\n",
    "num_classes= 2 #[abnormal, normal]\n",
    "input_shape = (img_width, img_height, channel)\n",
    "model_input = Input(shape=input_shape)\n",
    "print(model_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% declared data generators, allocate 20% of the training data for validation\n",
    "\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_data_dir, \n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=13,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=13,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(val_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "#check the class indices\n",
    "print(train_generator.class_indices)\n",
    "print(val_generator.class_indices)\n",
    "print(test_generator.class_indices)\n",
    "\n",
    "#true labels\n",
    "Y_test=test_generator.classes\n",
    "print(Y_test.shape)\n",
    "\n",
    "#convert test labels to categorical\n",
    "Y_test1=to_categorical(Y_test, num_classes=num_classes, dtype='float32')\n",
    "print(Y_test1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use a wide sequential CNN with strided separable convolutions for this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cnn(model_input):\n",
    "    x = SeparableConv2D(32, (5, 5), padding='same', strides=2, activation='relu')(model_input)\n",
    "    x = SeparableConv2D(64, (5, 5), padding='same', strides=2, activation='relu')(x)\n",
    "    x = SeparableConv2D(128, (5, 5), padding='same', strides=2, activation='relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=model_input, outputs=x, name='custom_cnn')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "custom_model = custom_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "custom_model.summary()\n",
    "\n",
    "#plot the model\n",
    "#plot_model(custom_model, to_file='custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the pretrained model architecture: models used in this study: VGG-16, VGG-19, Inception-V3, InceptionResnet-V2, Xception, DenseNet-201, MobileNet-v2, and NasNet-mobile. The truncated models are added with (a)zero-padding, (b) a strided separable convolutional layer with 5Ã—5 filters and 1024 feature maps, (c) GAP layer, (d) Dropout layer, and (e) final dense layer with Softmax activation. The models are trained to become modality-specific to be fine-tuned further on the multi-class dataset. We have shown here for the VGG-16 model that can be adapted for all pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = applications.VGG16(weights='imagenet', \n",
    "                                   include_top=False,\n",
    "                                   input_shape=(img_width,img_height,channel))\n",
    "#addind the top layers\n",
    "x = feature1_model.output\n",
    "x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = SeparableConv2D(1024, (5, 5), strides=2, activation='relu', name='extra_conv_vgg16')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=vgg16_cnn.input, outputs=predictions, name='vgg16_custom')\n",
    "model.summary()\n",
    "\n",
    "#enumerate and print layer names\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(train_generator.classes), \n",
    "                train_generator.classes)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% fix optimizer and start training the model\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True) #optimize to your requirements\n",
    "#compile the model\n",
    "model.compile(optimizer=sgd,              \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the generators first or it will give weird results\n",
    "train_generator.reset()\n",
    "val_generator.reset()\n",
    "\n",
    "# start training\n",
    "start = time.time()\n",
    "filepath = 'weights/' + model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n",
    "                             save_weights_only=False, \n",
    "                             save_best_only=True, mode='min', period=1)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=10, verbose=1, mode='min')\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='min', min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, tensor_board, earlyStopping, reduce_lr]\n",
    "\n",
    "custom_vgg16_history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=nb_train_samples // batch_size + 1, #see if not absolutely divisble by batch size\n",
    "      epochs=epochs,\n",
    "      validation_data=val_generator,\n",
    "      callbacks=callbacks_list,\n",
    "      class_weight = class_weights,\n",
    "      validation_steps=nb_validation_samples // batch_size + 1, \n",
    "      verbose=1)\n",
    "\n",
    "#print the total time taken for training\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot performance\n",
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"vgg16_performance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate with the best stored model weights\n",
    "model.load_weights('weights/vgg16_custom.14-0.9154.h5') #change this to your path and model weights\n",
    "model.summary()\n",
    "\n",
    "#compile the model\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True) #optimize to your requirements\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#predict\n",
    "test_generator.reset()\n",
    "custom_vgg16_y_pred = model.predict_generator(test_generator, \n",
    "                                              nb_test_samples/batch_size, workers=1)\n",
    "#true labels\n",
    "Y_test=test_generator.classes\n",
    "\n",
    "#print the shape of y_pred and Y_test\n",
    "print(custom_vgg16_y_pred.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "#measure accuracy\n",
    "custom_vgg16_model_accuracy=accuracy_score(Y_test,custom_vgg16_y_pred.argmax(axis=-1))\n",
    "print('The accuracy of custom VGG16 model is: ', custom_vgg16_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% #print classification report\n",
    "\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] #from the generator.class_indices\n",
    "print(classification_report(Y_test,\n",
    "                            custom_vgg16_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test, custom_vgg16_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for custom VGG16 model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store predictions: \n",
    "#predicted_class_indices has the predicted labels, 0 for abnormal and 1 for normal\n",
    "predicted_class_indices=np.argmax(custom_vgg16_y_pred,axis=1)\n",
    "\n",
    "#map the predicted labels with their unique ids \n",
    "#such as filenames to find out what you predicted for which image.\n",
    "labels = (train_generator.class_indices) # 0 for abnormal and 1 for normal\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices] #displays as string\n",
    "\n",
    "#Finally, save the results to a CSV file.\n",
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "results.to_csv(\"predictions_base_vgg16.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test1[:, i], custom_vgg16_y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "  \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test1.ravel(), custom_vgg16_y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(num_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= num_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "lw = 2\n",
    "fig=plt.figure(figsize=(15,10), dpi=70)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Major ticks every 0.05, minor ticks every 0.05\n",
    "major_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "\n",
    "#plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('multi-class ROC curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly train all the pretrained models and select the top-3 performing modlaity-specific models. In this case, VGG-16, VGG-19, and Inception-V3 are the top-3 performing models with modality specific knowledge transfer. we further finetuned these modesl to categorize the CXRs from the combined pediatric CXR and COVID-19 CXR collections (Twitter and Montreal) into CXRs as showing normal, bacterial pneumonia, and COVID-19 viral pneumonia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image dimensions and loading\n",
    "img_width, img_height = 256,256\n",
    "train_data_dir = '.../combined_pediatric_covid19/train' #path to your data\n",
    "test_data_dir = '.../combined_pediatric_covid19/test' #path to your data\n",
    "epochs = 32\n",
    "batch_size = 16\n",
    "num_classes = 3 #normal, bacterial and covid_viral\n",
    "input_shape = (img_width, img_height, 3)\n",
    "model_input = Input(shape=input_shape)\n",
    "print(model_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% declared data generators, allocate 20% of the training data for validation\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_data_dir, \n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(val_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "#check the class indices\n",
    "print(train_generator.class_indices)\n",
    "print(val_generator.class_indices)\n",
    "print(test_generator.class_indices)\n",
    "\n",
    "#true labels\n",
    "Y_test=test_generator.classes\n",
    "print(Y_test.shape)\n",
    "\n",
    "#convert test labels to categorical\n",
    "Y_test1=to_categorical(Y_test, num_classes=num_classes, dtype='float32')\n",
    "#print(Y_test1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load the modality-specific models, truncated them at the deepest zero padding layer and append with (a)zero-padding, (b) a strided separable convolutional layer with 5Ã—5 filters and 1024 feature maps, (c) GAP layer, (d) Dropout layer, and (e) final dense layer with Softmax activation. The models are finetuned to classify the data into normal, bacterial pneunmonia, and COVID-19 viral pneumonia classes. Here we demonstrate the process for the modality-specific VGG-16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_custom_model.load_weights('.../modality_specific/vgg16_custom.09-0.9683.h5')\n",
    "vgg16_custom_model.summary()\n",
    "#extract the features from block5_pool\n",
    "base_model_vgg16=Model(inputs=vgg16_custom_model.input,outputs=vgg16_custom_model.get_layer('block5_pool').output)\n",
    "#addind the top layers\n",
    "x = base_model_vgg16.output\n",
    "x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = Conv2D(1024, (5,5), strides = 2, activation='relu', name='extra_conv1_vgg16')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "model = Model(inputs=base_model_vgg16.input, outputs=predictions, name = 'vgg16_multiclass_finetuned')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "#make the base model trainable layers non -trainable\n",
    "for layer in base_model_vgg16.layers:\n",
    "    layer.trainable = False\n",
    "model.summary()\n",
    "   \n",
    "#plot the model\n",
    "plot_model(model, to_file='VGG16_multiclass_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(train_generator.classes), \n",
    "                train_generator.classes)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix optimizer and start training the model\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True) #optimize to your requirements\n",
    "#compile the model\n",
    "model.compile(optimizer=sgd,              \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the generators \n",
    "train_generator.reset()\n",
    "val_generator.reset()\n",
    "\n",
    "# start training\n",
    "start = time.time()\n",
    "filepath = 'weights/' + model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n",
    "                             save_weights_only=False, \n",
    "                             save_best_only=True, mode='min', period=1)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=10, verbose=1, mode='min')\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='min', min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, tensor_board, earlyStopping, reduce_lr]\n",
    "\n",
    "custom_vgg16_history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=nb_train_samples // batch_size + 1, #check if absolutely divisible or not\n",
    "      epochs=epochs,\n",
    "      validation_data=val_generator,\n",
    "      callbacks=callbacks_list,\n",
    "      class_weight = class_weights,\n",
    "      validation_steps=nb_validation_samples // batch_size + 1, \n",
    "      verbose=1)\n",
    "\n",
    "#print the total time taken for training\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot performance\n",
    "N = epochs #modify if early stopping\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"vgg16_multiclass_performance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate with the best stored model weights\n",
    "model.load_weights('weights/vgg16_multiclass_finetuned.04-0.9308.h5') #change this to your path and model weights\n",
    "model.summary()\n",
    "#compile the model\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True) #optimize to your requirements\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "vgg16_custom_y_pred = model.predict_generator(test_generator, \n",
    "                                            nb_test_samples // batch_size + 1, verbose=1)\n",
    "\n",
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),\n",
    "                          vgg16_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),\n",
    "                                vgg16_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),\n",
    "                                     vgg16_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),\n",
    "                               vgg16_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(bacterial)','class 1(covid19)', 'class 2(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),\n",
    "                            vgg16_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),\n",
    "                              vgg16_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),\n",
    "                                    vgg16_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, \n",
    "                                    figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# If yo want, plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "# linewidth\n",
    "lw = 2\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test1[:, i], vgg16_custom_y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "  \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test1.ravel(), vgg16_custom_y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(num_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= num_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "fig=plt.figure(figsize=(15,10), dpi=70)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Major ticks every 0.05, minor ticks every 0.05\n",
    "major_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "\n",
    "#plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in view of the upper left corner.\n",
    "fig=plt.figure(figsize=(15,10), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Major ticks every 0.05, minor ticks every 0.05\n",
    "major_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "\n",
    "#plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('multi-class ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative model purning: Repeat the above for the top-3 modality specific models to finetune them to perform multi-class classification. The best model weights are stored further for iterative pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Loading the training data\n",
    "img_width, img_height = 256,256\n",
    "train_data_dir = '.../combined_pediatric_covid19/train' \n",
    "test_data_dir = '.../combined_pediatric_covid19/test'\n",
    "epochs = 32\n",
    "batch_size = 16\n",
    "val_batch_size = 16\n",
    "num_classes = 3\n",
    "percent_pruning = 2 #amount of weights to prune from each layer\n",
    "total_percent_pruning = 50 #pruning up to 50% of original model weights, modify based on your maximum pruning percentage\n",
    "input_shape = (img_width, img_height, 3)\n",
    "model_input = Input(shape=input_shape)\n",
    "print(model_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define pruning functions: These functions are adapted from Keras Surgeon\n",
    "\n",
    "def get_total_channels(model):\n",
    "    start = None\n",
    "    end = None\n",
    "    channels = 0\n",
    "    for layer in model.layers[start:end]:\n",
    "        if layer.__class__.__name__ == 'Conv2D':\n",
    "            channels += layer.filters\n",
    "    return channels\n",
    "\n",
    "def get_model_apoz(model, generator):\n",
    "    # Get APoZ\n",
    "    start = None\n",
    "    end = None\n",
    "    apoz = []\n",
    "    for layer in model.layers[start:end]:\n",
    "        if layer.__class__.__name__ == 'Conv2D':\n",
    "            print(layer.name)\n",
    "            apoz.extend([(layer.name, i, value) for (i, value)\n",
    "                         in enumerate(get_apoz(model, layer, generator))])\n",
    "\n",
    "    layer_name, index, apoz_value = zip(*apoz)\n",
    "    apoz_df = pd.DataFrame({'layer': layer_name, 'index': index,\n",
    "                            'apoz': apoz_value})\n",
    "    apoz_df = apoz_df.set_index('layer')\n",
    "    return apoz_df\n",
    "\n",
    "def prune_model(model, apoz_df, n_channels_delete):\n",
    "    sorted_apoz_df = apoz_df.sort_values('apoz', ascending=False)\n",
    "    high_apoz_index = sorted_apoz_df.iloc[0:n_channels_delete, :]\n",
    "    surgeon = Surgeon(model, copy=True)\n",
    "    for name in high_apoz_index.index.unique().values:\n",
    "        channels = list(pd.Series(high_apoz_index.loc[name, 'index'],\n",
    "                                  dtype=np.int64).values)\n",
    "        surgeon.add_job('delete_channels', model.get_layer(name),\n",
    "                        channels=channels)\n",
    "    # Delete channels\n",
    "    return surgeon.operate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(train_generator.classes), \n",
    "                train_generator.classes)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform iterative pruning of the multiclass finetuned models to find the performing model with test data\n",
    "#here we demosntrate for the VGG16 multiclass finetuned model\n",
    " \n",
    "def iterative_prune_model():\n",
    "    model = load_model('.../multiclass/vgg16_multiclass_finetuned.07-0.9702.h5')\n",
    "    model.compile(optimizer=SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True),\n",
    "                             loss='categorical_crossentropy',\n",
    "                             metrics=['accuracy'])\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    train_steps = (train_generator.n // train_generator.batch_size) + 1 #see if absolutely divisible\n",
    "\n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255) \n",
    "    \n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=val_batch_size,\n",
    "        class_mode='categorical', \n",
    "        shuffle=False)\n",
    "    \n",
    "    val_steps = (validation_generator.n // validation_generator.batch_size) + 1\n",
    "    \n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(train_generator.classes), \n",
    "                train_generator.classes)\n",
    "    \n",
    "    # Evaluate the model performance before pruning\n",
    "    loss = model.evaluate_generator(validation_generator,\n",
    "                                    val_steps)\n",
    "    print('original model validation loss: ', loss[0], ', acc: ', loss[1])\n",
    "\n",
    "    #get total number of channels and remove 2% of channels with higheset APoZ\n",
    "    total_channels = get_total_channels(model)\n",
    "    n_channels_delete = int(math.floor(percent_pruning / 100 * total_channels))\n",
    "\n",
    "    # Incrementally prune the network, retraining it each time\n",
    "    percent_pruned = 0\n",
    "    # If percent_pruned > 0, continue pruning from previous checkpoint\n",
    "    if percent_pruned > 0:\n",
    "        checkpoint_name = ('.../pruning/multiclass/vgg16/' + \\\n",
    "                           'vgg16_pruning_' + str(percent_pruned)\n",
    "                           + 'percent')\n",
    "        model = load_model(checkpoint_name + '.h5')\n",
    "\n",
    "    while percent_pruned <= total_percent_pruning:\n",
    "        # Prune the model\n",
    "        apoz_df = get_model_apoz(model, validation_generator)\n",
    "        percent_pruned += percent_pruning\n",
    "        print('pruning up to ', str(percent_pruned),\n",
    "              '% of the original model weights')\n",
    "        model = prune_model(model, apoz_df, n_channels_delete)\n",
    "\n",
    "        # Clean up tensorflow session after pruning and re-load model\n",
    "        checkpoint_name = ('.../pruning/multiclass/vgg16/' + \\\n",
    "                           'vgg16_pruning_' + str(percent_pruned)\n",
    "                           + 'percent')\n",
    "        model.save(checkpoint_name + '.h5')\n",
    "        del model\n",
    "        K.clear_session()\n",
    "        tf.reset_default_graph()\n",
    "        model = load_model(checkpoint_name + '.h5')\n",
    "\n",
    "        # Re-train the model\n",
    "        model.compile(optimizer=SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True),\n",
    "                             loss='categorical_crossentropy',\n",
    "                             metrics=['accuracy'])\n",
    "        checkpoint_name = ('.../pruning/multiclass/vgg16/' + \\\n",
    "                           'vgg16_pruning_' + str(percent_pruned)\n",
    "                           + 'percent' + '.h5')\n",
    "        csv_logger = CSVLogger('.../pruning/multiclass/vgg16/' + \\\n",
    "                               'vgg16_pruning_' + str(percent_pruned)\n",
    "                           + 'percent' + '.csv')\n",
    "        checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose=1, \n",
    "                             save_weights_only=False, \n",
    "                             save_best_only=True, mode='min', period=1)\n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=5, verbose=1, mode='min')\n",
    "        tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, \n",
    "                                   batch_size=batch_size)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3,\n",
    "                              verbose=1, mode='min', min_lr=0.00001)\n",
    "        model.fit_generator(train_generator,\n",
    "                            steps_per_epoch=train_steps,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=val_steps,\n",
    "                            workers=4,\n",
    "                            class_weight = class_weights,\n",
    "                            callbacks=[csv_logger,checkpoint, \n",
    "                                       tensor_board, earlyStopping, reduce_lr\n",
    "                                       ])\n",
    "\n",
    "    # Evaluate the final model performance\n",
    "    loss = model.evaluate_generator(validation_generator,\n",
    "                                    validation_generator.n //\n",
    "                                    validation_generator.batch_size)\n",
    "    print('pruned model loss: ', loss[0], ', acc: ', loss[1])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    iterative_prune_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the top-performing model from the iteratively pruned set for each of the fine-tuned model., viz. VGG-16, VGG-19 and Inception-V3 to perform ensembles. Here we demosntrate the pruned VGG-16 mdoels performance with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top performer: 20% channel pruned model gave the top performance with the test data\n",
    "model_1 = load_model('...pruned/vgg16_pruning_20percent.h5')\n",
    "model_1.summary()\n",
    "\n",
    "#compile\n",
    "model_1.compile(optimizer=SGD(lr=0.001, decay=1e-6, \n",
    "                              momentum=0.95, nesterov=True),\n",
    "                             loss='categorical_crossentropy',\n",
    "                             metrics=['accuracy'])\n",
    "\n",
    "# Make predictions\n",
    "print('-'*30)\n",
    "print('Predicting on test data...')\n",
    "print('-'*30)\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "y_pred_vgg16 = model_1.predict_generator(test_generator, \n",
    "                                         (test_generator.n //\n",
    "                                test_generator.batch_size) + 1,\n",
    "                                verbose=1)\n",
    "\n",
    "accuracy_m1 = accuracy_score(Y_test,y_pred_vgg16.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print classification report\n",
    "target_names = ['class 0(bacterial)','class 1(covid19)', 'class 2(normal)'] \n",
    "print(classification_report(Y_test,y_pred_vgg16.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,y_pred_vgg16.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test,y_pred_vgg16.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(15,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=70)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other metrics\n",
    "#The F-beta score is the weighted harmonic mean of precision and recall, \n",
    "#reaching its optimal value at 1 and its worst value at 0.\n",
    "#The beta parameter determines the weight of recall in the combined score. \n",
    "#beta < 1 lends more weight to precision, while beta > 1 favors recall \n",
    "#(beta -> 0 considers only precision, beta -> +inf only recall).\n",
    "fbeta = fbeta_score(Y_test,y_pred_vgg16.argmax(axis=-1), average='macro', beta=1.5)\n",
    "print('The fbeta-score of the Custom model is: ', fbeta)\n",
    "\n",
    "#The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "#where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "#The relative contribution of precision and recall to the F1 score are equal.\n",
    "f1 = f1_score(Y_test,y_pred_vgg16.argmax(axis=-1), average='weighted')\n",
    "print('The f1-score of the Custom model is: ', f1)\n",
    "\n",
    "prec = precision_score(Y_test,y_pred_vgg16.argmax(axis=-1), \n",
    "                       average='weighted') #can be macro or weighted\n",
    "print('The precision of the Custom model is: ', prec)\n",
    "\n",
    "rec = recall_score(Y_test,y_pred_vgg16.argmax(axis=-1), \n",
    "                   average='weighted')\n",
    "print('The recall of the Custom model is: ', rec)\n",
    "\n",
    "mat_coeff = matthews_corrcoef(Y_test,y_pred_vgg16.argmax(axis=-1))\n",
    "print('The MCC of the Custom model is: ', mat_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the ROC-AUC values\n",
    "lw = 2\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test1[:, i], y_pred_vgg16[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "  \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test1.ravel(), y_pred_vgg16.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(num_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= num_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "fig=plt.figure(figsize=(15,10), dpi=70)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Major ticks every 0.05, minor ticks every 0.05\n",
    "major_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "\n",
    "#plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in view of the upper left corner.\n",
    "fig=plt.figure(figsize=(15,10), dpi=70)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Major ticks every 0.05, minor ticks every 0.05\n",
    "major_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "\n",
    "#plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multi-class ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENSEMBLE: We will begin the ensemble process, for confusion matrix, ROC-AUC and other metrics calculation, follow the previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#majority voting\n",
    "#lets perform dummy assignment of the predictions for majority voting\n",
    "y_pred_vgg161 = y_pred_vgg16\n",
    "y_pred_vgg191 = y_pred_vgg19\n",
    "y_pred_inceptionv31 = y_pred_inceptionv3\n",
    "\n",
    "#perform majority voting of the top-3 models\n",
    "y_pred_vgg161 = y_pred_vgg161.argmax(axis=-1)\n",
    "y_pred_vgg191 = y_pred_vgg191.argmax(axis=-1)\n",
    "y_pred_inceptionv31 = y_pred_inceptionv31.argmax(axis=-1)\n",
    "\n",
    "#max voting begins\n",
    "max_voting_top3 = np.array([])\n",
    "\n",
    "for i in range(0,len(test_generator.filenames)):\n",
    "    max_voting_top3 = np.append(max_voting_top3, \n",
    "                                statistics.mode([y_pred_vgg161[i],\n",
    "                                                 y_pred_vgg191[i],\n",
    "                                                 y_pred_inceptionv31[i]])) \n",
    "\n",
    "ensemble_max_voting_top_3_accuracy = accuracy_score(Y_test,max_voting_top3)\n",
    "\n",
    "print('The acurracy of the top-3 iteratively pruned ensemble is ', \\\n",
    "      ensemble_max_voting_top_3_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple averaging\n",
    "#lets perform dummy assignment\n",
    "y_pred_vgg161 = y_pred_vgg16\n",
    "y_pred_vgg191 = y_pred_vgg19\n",
    "y_pred_inceptionv31 = y_pred_inceptionv3\n",
    "\n",
    "#top-3:\n",
    "simple_averaging_top_3 = (y_pred_vgg161 + y_pred_vgg191 + \\\n",
    "                          y_pred_inceptionv31)/3\n",
    "\n",
    "#accuracy measure:\n",
    "ensemble_simple_averaging_top_3_accuracy = accuracy_score(Y_test,\n",
    "                                                          simple_averaging_top_3.argmax(axis=-1))\n",
    "\n",
    "print('The simple averaging acurracy of the top-3 iteratively pruned ensemble is ', \\\n",
    "      ensemble_simple_averaging_top_3_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% #weighted averaging\n",
    "'''This is an extension of the averaging method. \n",
    "All models are assigned different weights defining the importance of each model for prediction. \n",
    "With the top-3 models, the Inceptionv3 model gave superior performance\n",
    "followed by VGG-19 and VG-16, hence we allocate weights of 0.5, 0.4 and 0.1\n",
    "based on empirical observations\n",
    "'''\n",
    "#dummy assignment\n",
    "y_pred_vgg161 = y_pred_vgg16\n",
    "y_pred_vgg191 = y_pred_vgg19\n",
    "y_pred_inceptionv31 = y_pred_inceptionv3\n",
    "\n",
    "#weighted averaging\n",
    "weighted_averaging_top_3 = (y_pred_vgg161*0.1 + y_pred_vgg191*0.4 + \\\n",
    "                            y_pred_inceptionv31*0.5)/3\n",
    "#accuracy measure:\n",
    "ensemble_weighted_averaging_top_3_accuracy = accuracy_score(Y_test,\n",
    "                                                            weighted_averaging_top_3.argmax(axis=-1))\n",
    "\n",
    "print('The weighted averaging acurracy of the top-3 iteratively pruned ensemble is ', \\\n",
    "      ensemble_weighted_averaging_top_3_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STACKED GENERALIZATION: we are going to perform a stacking ensemble of the top-3 pruned models to see if it improves performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the pruned models\n",
    "\n",
    "def vgg16_prune(model_input):\n",
    "    vgg16_prune = load_model('.../pruned/vgg16_pruning_20percent.h5')\n",
    "    x = vgg16_prune.output\n",
    "    model = Model(inputs=vgg16_prune.input, outputs=x, name='vgg16_prune')\n",
    "    return model\n",
    "vgg16_prune_model = vgg16_prune(model_input)\n",
    "vgg16_prune_model.summary()\n",
    "\n",
    "def vgg19_prune(model_input):\n",
    "    vgg19_prune = load_model('.../pruned/vgg19_pruning_18percent.h5')\n",
    "    x = vgg19_prune.output\n",
    "    model = Model(inputs=vgg19_prune.input, outputs=x, name='vgg19_prune')\n",
    "    return model\n",
    "vgg19_prune_model = vgg19_prune(model_input)\n",
    "vgg19_prune_model.summary()\n",
    "\n",
    "def inceptionv3_prune(model_input):\n",
    "    inceptionv3_prune = load_model('.../pruned/inceptionv3_pruning_32percent.h5')\n",
    "    x = inceptionv3_prune.output\n",
    "    model = Model(inputs=inceptionv3_prune.input, outputs=x, name='inceptionv3_prune')\n",
    "    return model\n",
    "inceptionv3_prune_model = inceptionv3_prune(model_input)\n",
    "inceptionv3_prune_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the instantiated models\n",
    "n_models = 3 #we have three pruned models\n",
    "\n",
    "def load_all_models(n_models):\n",
    "    all_models = list()\n",
    "    vgg16_prune_model.load_weights('.../pruned/vgg16_pruning_20percent.h5')\n",
    "    all_models.append(vgg16_prune_model)\n",
    "    vgg19_prune_model.load_weights('.../pruned/vgg19_pruning_18percent.h5')\n",
    "    all_models.append(vgg19_prune_model)\n",
    "    inceptionv3_prune_model.load_weights('.../pruned/inceptionv3_pruning_32percent.h5')\n",
    "    all_models.append(inceptionv3_prune_model)\n",
    "    return all_models\n",
    "\n",
    "# load models\n",
    "n_members = 3\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate standalone models on test dataset\n",
    "\n",
    "for model in members:\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True) \n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    _, acc = model.evaluate_generator(test_generator, \n",
    "                                      (test_generator.n //test_generator.batch_size) + 1,\n",
    "                                      verbose=1)\n",
    "    print('Model Accuracy: %.4f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated Stacking Model: It may be desirable to use a neural network as a meta-learner.Specifically, the sub-networks can be embedded in a larger multi-headed neural network that then learns how to best combine the predictions from each input sub-model. It allows the stacking ensemble to be treated as a single large model. The benefit of this approach is that the outputs of the submodels are provided directly to the meta-learner. Further, it is also possible to update the weights of the submodels in conjunction with the meta-learner model, if this is desirable. The outputs of each of the models can then be merged. In this case, we will use a simple concatenation merge, where a single 9-element vector will be created from the three class-probabilities predicted by each of the 3 models. We will then define a hidden layer to interpret this â€œinputâ€ to the meta-learner and an output layer that will make its own probabilistic prediction. A plot of the network graph is created when this function is called to give an idea of how the ensemble model fits together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_stacked_model(members):\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "        # make not trainable\n",
    "            layer.trainable = False\n",
    "            # rename to avoid 'unique layer name' issue\n",
    "            layer.name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = Dense(9, activation='relu')(merge) # three ouputs for 3 models, so 9 hidden neurons\n",
    "    output = Dense(3, activation='softmax')(hidden) #three classes\n",
    "    model = Model(inputs=ensemble_visible, outputs=output, name = 'stacking_ensemble_pruned')    \n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    return model\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "stacked_model.summary()\n",
    "\n",
    "#plot model\n",
    "#plot_model(stacked_model, to_file='stacked_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is defined, it can be fit. We can fit it directly on the holdout validation dataset. Because the sub-models are not trainable, their weights will not be updated during training and only the weights of the new hidden and output layer will be updated. The stacking neural network model will be fit on the trainig data for 32 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the ensemble model\n",
    "filepath = '.../weights/' + stacked_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "history = stacked_model.fit_generator(train_generator, \n",
    "                                      steps_per_epoch=train_steps,\n",
    "                                      epochs=32, \n",
    "                                      validation_data=test_generator,\n",
    "                                      class_weight = class_weights,\n",
    "                                      callbacks=callbacks_list,\n",
    "                                      validation_steps=(test_generator.n //test_generator.batch_size) + 1, #see if absolutely divisible\n",
    "                                      verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot performance of the ensemble model\n",
    "N = 32 # see if early stopping\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\".../stacking_ensemble_pruned.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once fit, we can use the new stacked model to make a prediction on new data.\n",
    "#load the best model\n",
    "stacked_model.load_weights('.../stacking_ensemble_pruned.17-0.9663.h5')\n",
    "stacked_model.summary()\n",
    "\n",
    "#first reset the test generator \n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "ensemble_y_pred = stacked_model.predict_generator(test_generator, \n",
    "                                                  (test_generator.n //test_generator.batch_size) + 1,\n",
    "                                                  verbose=1)\n",
    "\n",
    "#print prediction shapes\n",
    "print(ensemble_y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure performance metrics of the stacked ensemble\n",
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),\n",
    "                          ensemble_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),\n",
    "                                ensemble_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),\n",
    "                                     ensemble_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),\n",
    "                               ensemble_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print classification report and plot confusion matrix\n",
    "target_names = ['class 0(bacterial)','class 1(covid19)', 'class 2(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),\n",
    "                            ensemble_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),\n",
    "                              ensemble_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "lw = 2\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test1[:, i], y_pred_vgg16[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "  \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test1.ravel(), y_pred_vgg16.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(num_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= num_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "fig=plt.figure(figsize=(15,10), dpi=70)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Major ticks every 0.05, minor ticks every 0.05\n",
    "major_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "\n",
    "#plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('multi-class ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in view of the upper left corner.\n",
    "fig=plt.figure(figsize=(15,10), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Major ticks every 0.05, minor ticks every 0.05\n",
    "major_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('multi-class ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT precision-recall curves\n",
    "\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=100)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        ensemble_y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], ensemble_y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   ensemble_y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, ensemble_y_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tabulate the performance of the ensembles using the above methods and compare to find the best performaing ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fuyrther perform visualiztion studies of the top-3 pruned models to see if they are loclaizing the salinet ROI iinvolved in clinical decision making. Here, we demonstrate for a VGG-16 pruned model using Grad-CAM and LIME visualization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model = load_model('vgg16_pruning_20percent.h5')\n",
    "model.summary()\n",
    "\n",
    "#compile the model\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True) #optimize to your requirements\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to image to visualize\n",
    "img_path = 'ryct.2020200034.fig5-day4.png' #path to your image\n",
    "img = image.load_img(img_path)\n",
    "\n",
    "#preprocess the image\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x /= 255 \n",
    "\n",
    "#predict on the image\n",
    "preds = model.predict(x)[0]\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begin visualization\n",
    "covid_output = model.output[:, 1] #index 1 for the COVID-19 CLASS image\n",
    "\n",
    "#Output feature map from the last convolutional layer\n",
    "last_conv_layer = model.get_layer('block5_conv3')\n",
    "\n",
    "#compute the Gradient of the expected class with regard to the output feature map of block5_conv3 \n",
    "#(or the deepst convolutional layer)\n",
    "grads = K.gradients(covid_output, last_conv_layer.output)[0]\n",
    "\n",
    "#Vector of shape (512,), where each entry is the mean intensity of the gradient over a specific feature-map channel\n",
    "pooled_grads = K.mean(grads, axis=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access the values of the quantities we defined: \n",
    "#pooled_grads and the output feature map of block5_conv3, given a sample image\n",
    "iterate = K.function([model.input],[pooled_grads, last_conv_layer.output[0]])\n",
    "\n",
    "#Values of these two quantities, as Numpy arrays, given the sample image\n",
    "pooled_grads_value, conv_layer_output_value = iterate([x])\n",
    "\n",
    "#Multiplies each channel in the feature-map array by â€œhow important this channel isâ€ with regard to the expected class\n",
    "for i in range(512): #number of filters in the deepest convolutional layer\n",
    "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "\n",
    "#For visualization purposes, we normalize the heatmap between 0 and 1.\n",
    "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)\n",
    "plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from stack overflow for writng the png output with different DPIs.\n",
    "def writePNGwithdpi(im, filename, dpi=(72,72)):\n",
    "   \"\"\"Save the image as PNG with embedded dpi\"\"\"\n",
    "   # Encode as PNG into memory\n",
    "   retval, buffer = cv2.imencode(\".png\", im)\n",
    "   s = buffer.tostring()\n",
    "   # Find start of IDAT chunk\n",
    "   IDAToffset = s.find(b'IDAT') - 4\n",
    "   pHYs = b'pHYs' + struct.pack('!IIc',int(dpi[0]/0.0254),int(dpi[1]/0.0254),b\"\\x01\" ) \n",
    "   pHYs = struct.pack('!I',9) + pHYs + struct.pack('!I',zlib.crc32(pHYs))\n",
    "   with open(filename, \"wb\") as out:\n",
    "      out.write(buffer[0:IDAToffset])\n",
    "      out.write(pHYs)\n",
    "      out.write(buffer[IDAToffset:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#superimporse heatmap on the original image\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "#Resizes the heatmap to be the same size as the original image\n",
    "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "#Converts the heatmap to RGB \n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "#Applies the heatmap to the original image\n",
    "#0.4 here is a heatmap intensity factor.\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "superimposed_img = heatmap * 0.4 + img \n",
    "\n",
    "#Saves the image to disk\n",
    "cv2.imwrite('covid_cam_vgg16_fourthimage.jpg', superimposed_img)\n",
    "\n",
    "if we have to increse the DPI and write to disk\n",
    "writePNGwithdpi(superimposed_img, \"image_grad_cam.png\", (300,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use LIME visualization to interpret the models behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the explainer\n",
    "explainer = lime_image.LimeImageExplainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the parameters to modify: \n",
    "hide_color- is the color for a superpixel turned OFF. Alternatively, if it is NONE, the superpixel will be replaced by the average of its pixels. Here, we set it to 0 (means gray).\n",
    "top_labels - if not None, ignore labels and produce explanations for the K labels with highest prediction probabilities, where K is this parameter.\n",
    "First, we generate neighborhood data by randomly perturbing features from the instance \n",
    "labels â€“ iterable with labels to be explained.\n",
    "num_features â€“ maximum number of features present in explanation\n",
    "num_samples â€“ size of the neighborhood to learn the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer.explain_instance(x[0], \n",
    "                                         model.predict, top_labels=1, #produces explanations for the top 1 label\n",
    "                                         num_features = 10, hide_color=0, num_samples=42)\n",
    "print(explanation.top_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wall time: more than 3 minutes in local windows system. \n",
    "Explanation: for the top-class. We can see the top 10 superpixels that are most positive towards the class with the rest of the image shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], #change the respective class index\n",
    "                                            positive_only=False, \n",
    "                                            num_features=5, hide_rest=False) \n",
    "plt.figure()\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "plt.figure()\n",
    "plt.imshow(x[0] / 2 + 0.5) #this increases te brightness of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
